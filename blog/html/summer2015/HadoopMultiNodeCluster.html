<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Cluster Setup</title>

    <!-- Bootstrap Core CSS -->
    <link href="../../css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="../../css/clean-blog.min.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body>

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-custom navbar-fixed-top">
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="../../../index.php">metastableB</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="../../../index.php">Home</a>
                    </li>
                    <li>
                        <a href="../../index.html">Blog</a>
                    </li>
                    <!--<li>
                        <a href="post.html">Sample Post</a>
                    </li>!-->
                    <li>
                        <a href="../../contact.html">Contact</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header class="intro-header" style="background-image: url('../../img/firstpage-bg.jpg')">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <div class="post-heading">
                        <h1>Hadoop Yarn: Setting Up A Multinode Cluster</h1>
                        <span class="meta">Last Update on May 2, 2015</span>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- Post Content -->
    <article>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <h3 style="font-size: 50px;color: rgb(103, 123, 83);"></h3>
                    
                    <p>For one of the summer projects I did in the summer of 2015, I had to work on a multi node cluster setup of hadoop. Hadoop and clusters were something totally new to me and setting everything up was a slightly difficult task, and through the process I was able to learn a lot. Here I'll try to document the steps I took to set up a cluster. This article is intended for all those who are trying to learn how to use hadoop over a cluster and experiment with it. This does not, in any way cover all aspects of the installation, the different configuration options, their impact , the security aspects etc. What you'll have here is an stepping stone towards your ventures into the BigData field.</p>

				   <h4 style="color: rgb(34, 237, 122); font-size: 25px;">0 Background Information</h4>

				   <p>The hadoop cluster set up requires a number of option which depend on you particular case. I had access to 10VMs for my project, all with a clean install of ubuntu 14.04 on them. You can adapt the following with slight modifications to any Linux distro of your choice.</p>

				    <p> Also, I did all my operations by SSH-ing into the VMs from my home machine which I did not make a part of the cluster itself. Though the same method applies even if you want it to be a part of the cluster.</p>

                    <h4 style="color: rgb(34, 237, 122); font-size: 25px;">1&nbsp;:&nbsp;Initial Setup</h4>

                    <p> As mentioned above I used
                    	<ul>
                    		<li>Linux (ubuntu 14.04)</li>
                    		<li>Hadoop 2.7.0 (yarn) </li>
                    		<li>java 1.8.0_45</li>
                    	</ul> 
                    Though this tutorial will still be valid for any Linux distro and Hadoop Yarn version (2.x). </p>

                    <p>The usual set-up first designates one of the node as NameNode and others as DataNodes. Since I had 10 VMs, I used 1 of them as a NameNode (as well as the resource manager as we will see later) and 8 others as DataNode. I sat aside 1 VM as a single node setup for other testing purposes (irrelevant here).</p>

                    <p>In case you have fewer node, you might want to run a DataNode daemon on you NameNode machine as well. I'll mention below what all the changes you will have to make (there is just one change actually). </p>

                    The IPs of my machines are given below and my configuration will be with respect to these IPs. <b style="color:red">NOTE: Make sure to change the IPs according to your own machines</b>
                    <pre>192.168.1.241&nbsp;&nbsp;&nbsp;&nbsp;Master (NameNode)<br>192.168.1.242&nbsp;&nbsp;&nbsp;&nbsp;Slave0 (DataNode)<br>192.168.1.243&nbsp;&nbsp;&nbsp;&nbsp;Slave1 (DataNode)<br>192.168.1.244&nbsp;&nbsp;&nbsp;&nbsp;Slave2 (DataNode)<br>192.168.1.245&nbsp;&nbsp;&nbsp;&nbsp;Slave3 (DataNode)<br>192.168.1.246&nbsp;&nbsp;&nbsp;&nbsp;Slave4 (DataNode)<br>192.168.1.247&nbsp;&nbsp;&nbsp;&nbsp;Slave5 (DataNode)<br>192.168.1.248&nbsp;&nbsp;&nbsp;&nbsp;Slave6 (DataNode)<br>192.168.1.249&nbsp;&nbsp;&nbsp;&nbsp;Slave7 (DataNode)</pre>

	                <p>
                    <h5>1.1 &nbsp;:&nbsp;Adding System group and user account for all nodes</h5>
                    </p>

                    <p>Adding a new system group and user is recommended so that your hadoop setup and filesytem remain isolated from other system users. You can skip this step if you are absolutely sure this wont be necessary ( I personally did not bother do set this up, again not recommended).
                    </p>
                    <p>To set up a new user hadoop in the hduser, follow the following commands in all nodes<p>
                    <pre>sudo addgroup hadoop  
					<br>sudo adduser --ingroup hadoop hduser  
					<br>sudo adduser hduser sudo</pre>
					</p>
					<p>Make sure you understand the above commands and what they are doing. For all the next steps log in as 'hadoop' on all nodes</p>

					<p>
					<h5>1.1 &nbsp;:&nbsp;Mapping the nodes</h5>
                    </p>
                    <p>We will add the node information by adding the IPs of all the nodes (including the local node) to the <i>/etc/hosts</i> file of each node.</p>
                    Open your hosts file for editing.
                    <br>
                    <pre>$ sudo nano /etc/hosts</pre>
                    <br> Append the following : One Entry per line<br>
                    <pre>192.168.1.241&nbsp;&nbsp;&nbsp;Master<br>192.168.1.242&nbsp;&nbsp;&nbsp;Slave0<br>192.168.1.243&nbsp;&nbsp;&nbsp;Slave1<br>192.168.1.244&nbsp;&nbsp;&nbsp;Slave2<br>192.168.1.245&nbsp;&nbsp;&nbsp;Slave3<br>192.168.1.246&nbsp;&nbsp;&nbsp;Slave4<br>192.168.1.247&nbsp;&nbsp;&nbsp;Slave5<br>192.168.1.248&nbsp;&nbsp;&nbsp;Slave6<br>192.168.1.249&nbsp;&nbsp;&nbsp;Slave7</pre>

                    <p> Note : Ubuntu is notorious for having the 127&nbsp;.&nbsp;0&nbsp;.&nbsp;1&nbsp;.&nbsp;1 entry mapped to the hostname of your machine in /etc/hosts . Replace it with the network IP of your host. Do not touch the local host entry though.<br><br>Also make sure all your machines have different names on the network. Usually while creating VMs, one machine is created and replicated to the number of times you want. This may lead to all of them having the same name on the network. To edit the machine name of the network, open <i> /etc/hostname </i>file with your favourite editor and replace the current machine name (which you will find there) with the new name you want. Make sure to use this in the <i>/etc/hosts</i> for all the nodes as well.</p> 

                    <p>
                    <h5>1.3 &nbsp;:&nbsp;Password-less SSH</h5>
                    </p>
                    <p>It is better if we allow the namenode (master) to SSH into all the datanodes without a password. The scripts used to monitor and modify the hadoop daemons will be easier to handle with this. Also we require all the nodes to be able to SSH into itself.</p>

                    <p>To enable password-less login, on all nodes, perform the following command on all nodes</p>
                    <pre>$ ssh-keygen -t rsa <br>$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@Master<br>$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@Slave0<br>$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@Slave1<br>$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@Slave2<br>$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@Slave3<br>$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@Slave4<br>$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@Slave5<br>$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@Slave6<br>$ ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop@Slave7<br>$ chmod 0600 ~/.ssh/authorized_keys</pre>
                    <p>Make sure to modify the commands as your case may require. You'll have to replace the 'hadoop@Slave0' type part in each line. The argument form is [username]@[host], as you would do while logging in via SSH.
                    </p>
                    <p>
                    <h4 style="color: rgb(34, 237, 122); font-size: 25px;">2&nbsp;:&nbsp;Java</h4>

                    </p>
                    <p>Hadoop required java on your system. Type <i>java -version</i> on your terminal to see if you have java already installed. If the java version is echoed to the console, you can skip the downloading part. Though you still need to verify if the environment variables are set. Else download and setup java on all your nodes as instructed below. Actually you may find it easier to download java on one node and copy it the others. Use the linux tool <i>scp</i> for that. Even still you have to correctly set the environment variables</p>

                    <p><a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" style="color: rgb(255, 207, 40);">Download</a> the latest jdk. Extract it  within the home folder (or any location of your choice). I prefer to keep java in my <i>/usr/local/</i> directory. First extract the .tar.gz file you downloaded to a folder in your home directory, say jdk1.8.0_45, and then move it to your /usr/local directory. (or where ever you chose to move it to, but its better to stick to this directory).</p>
                    Downloading java
                    <pre>$ wget http://download.oracle.com/otn-pub/java/jdk/8u45-b14/jdk-8u45-linux-x64.tar.gz</pre>
                    Extracting it
                    <pre>tar xvzf jdk-8u45-linux-x64.tar.gz</pre>
                    Moving to the /user/local directory.
                    <pre>sudo mv -v /home/hadoop/jdk1.8.0_45 /usr/local</pre>
                    The -v flag for verbose output (prints a lot on the screen as you will see).<br>
                    To copy to the other nodes, use scp as follows
                    <pre>$scp -v /home/hadoop/Downloads/jdk-8u45-linux-x64.tar.gz hadoop@Slave0:~/Downloads/</pre>
                    
                    <p>Once you have java, you need to tell your environment that you have java. For that we set two environment variables, namely <i>$PATH</i> and <i>$JAVA_HOME</i>. We export the java installation directory and set the environment variables.</p>

                    <pre>export JAVA_HOME=/usr/local/jdk1.8.0_45<br>export PATH=$PATH:/usr/local/jdk1.8.0_45/bin:/usr/local/jdk1.8.0_45/sbin</pre>

                    <p>Remember that these changes are not persistent and are lost once the current terminal session is over. To have persistent effect, we have to add these lines to the shell profile file. In ubuntu this is the ~/.bashrc file. Open this file in your favourite editor and append the above two lines to the end. </p>
                    <pre>nano ~./bashrc <br># Append the following to the end<br>export JAVA_HOME=/usr/local/jdk1.8.0_45<br>export PATH=$PATH:/usr/local/jdk1.8.0_45/bin:/usr/local/jdk1.8.0_45/sbin</pre>

                    Make sure to change the installation path according to where you have set java.
                    <p>Now check if java is installed correctly on your machine with the following command. It should print the java version. You may have to restart the terminal session for your changes to the .bashrc file (if any) to take effect.</p>
                    <p><pre>java -version</pre></p>
                    
                    <h4 style="color: rgb(34, 237, 122); font-size: 25px;">3&nbsp;:&nbsp;Downloading a Hadoop</h4>
                    <p>Download the latest hadoop stable version form the <a href="http://apache.claz.org/hadoop/common/" style="color: rgb(255, 207, 40);">apache webpage</a> to your master node and one slave node. Extract your download to a directory of your choice within the file system. A very common and highly recommended choice is /opt/. Extract the contents of the file you downloaded to a new folder in /opt/ named hadoop. Make sure the user we created above, hadoop, has read write permissions to this folder.</p>

                    To download the file you can use
                    <pre>$wget http://apache.claz.org/hadoop/common/hadoop-2.7.0/hadoop-2.7.0.tar.gz</pre>
                    To extract the contents of the tar.gz file use
                    <pre>$tar xvzf hadoop-2.7.0.tar.gz </pre>
                    To move the extracted contents to /opt/hadoop use
                    <pre>$mv -v hadoop-2.7.0/* opt/hadoop</pre>
                    To copy to other nodes use,
                    <pre>$scp -v /your/hadoop/tar.gzfile hadoop@Slave0:~/Downloads</pre>

                   	<p>Now, as with java, we have to tell the environment that we have setup hadoop. First we set up the $HADOOP_HOME, $PATH environment variables. As done above for java,export the hadoop installation directory and set the environment variables</p>

                    <pre>export HADOOP_HOME=/opt/hadoop<br>export PATH=$PATH:/opt/hadoop/bin:/opt/hadoop/sbin</pre>

                    <p>Again, remember that these changes are not persistent and are lost once the current terminal session is over. To have persistent effect, we have to add these lines to the shell profile file. In ubuntu this is the ~/.bashrc file. Open this file in your favourite editor and append the above two lines to the end. </p>
                    <pre>nano ~./bashrc <br># Append the following to the end<br>export HADOOP_HOME=/opt/hadoop<br>export PATH=$PATH:/opt/hadoop/bin:/opt/hadoop/sbin</pre>

                    <p>Verify the hadoop installation on all nodes by printing the hadoop version</p>
                    <pre>$hadoop version</pre>

                    Add the remaining environmental variables as well to the .bashrc file. Append the following to the .bashrc file using your favourite editor.
                    <pre>export HADOOP_INSTALL=/opt/hadoop<br>export PATH=$PATH:$HADOOP_INSTALL/bin<br>export PATH=$PATH:$HADOOP_INSTALL/sbin<br>export HADOOP_MAPRED_HOME=$HADOOP_INSTALL<br>export HADOOP_COMMON_HOME=$HADOOP_INSTALL<br>export HADOOP_HDFS_HOME=$HADOOP_INSTALL<br>export YARN_HOME=$HADOOP_INSTALL<br>export HADOOP_HOME=$HADOOP_INSTALL<br>export HADOOP_CONF_DIR=${HADOOP_HOME}"/etc/hadoop"</pre>

                    <h4 style="color: rgb(34, 237, 122); font-size: 25px;">4&nbsp;:&nbsp;Configuring Hadoop</h4>
                    <p>
                    <h5>4.1 &nbsp;:&nbsp;Configuring the namenode</h5>
                    </p>
                    <p>These modifications are to be done only to the namenode. </p>
                    Make the following directories for logging and transaction data storage.
                    <pre>$mkdir -pv $HADOOP_INSTALL/data/namenode<br>$mkdir -pv $HADOOP_INSTALL/logs</pre>

                    <p>First edit the hdfs-site.xml file in $HADOOP_CONF_DIR and replace the content with the following. ( The property tag goes within the configuration tag. Make sure you use the correct IPs).
                    		</p>
                    <pre><xmp><configuration>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///opt/hadoop/data/namenode</value>
        <description>NameNode directory for namespace and transaction logs storage.</description>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <property>
        <name>dfs.permissions</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.datanode.use.datanode.hostname</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
        <value>false</value>
    </property>
    <property>
         <name>dfs.namenode.http-address</name>
         <value>192.168.1.241:50071</value>
         <description>Your NameNode hostname for http access.</description>
    </property>
    <property>
         <name>dfs.namenode.secondary.http-address</name>
         <value>192.168.1.241:50090</value>
         <description>Your Secondary NameNode hostname for http access.</description>
    </property>
</configuration></xmp></pre>

				<p>Now edit the $HADOOP_CONF/slaves file to tell the namenode what the IPs of the data node are. Include the IP of the namenode here as well if you want to run a datanode on it. Edit the file with your favourite editor and add one Ip per line</p>

				<pre>192.168.1.242<br>192.168.1.243<br>192.168.1.244<br>192.168.1.245<br>192.168.1.246<br>192.168.1.247<br>192.168.1.248<br></pre>

				<p>Next step is to edit the $HADOOP_CONF/yarn-site.xml file. Replace with the following content.</p>

				<pre><xmp><configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>192.168.1.241</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    <property>
        <name>yarn.resourcemanager.resource-tracker.address</name>
        <value>192.168.1.241:8025</value>
    </property>
    <property>
        <name>yarn.resourcemanager.scheduler.address</name>
        <value>192.168.1.241:8026</value>
    </property>
    <property>
        <name>yarn.resourcemanager.address</name>
        <value>192.168.1.241:8050</value>
    </property>
</configuration>
</xmp></pre>
<p>Again, remember to use your own IPs. Also, it is better if you understand what the properties actually mean. This will help debugging in case you encounter any errors. Check out the official API for more information.</p>

				 	<p>
                    <h5>4.2 &nbsp;:&nbsp;Configuring the datanode</h5>
                    </p>

                    <p>As you would expect, these configurations are exclusively for the datanodes. Edit the corresponding files in the $HADOOP_CONF_DIR directory and replace them with the contents here</p>
                    <p>But first create the log directories as above</p>
                    <pre>$mkdir -pv $HADOOP_INSTALL/data/datanode<br>$mkdir -pv $HADOOP_INSTALL/logs</pre>

                    $HADOOP_CONF_DIR/hdfs-site.xml
                    <pre><xmp><configuration>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///opt/hadoop/data/datanode</value>
        <description>DataNode directory</description>
    </property>

    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>

    <property>
        <name>dfs.permissions</name>
        <value>false</value>
    </property>

    <property>
        <name>dfs.datanode.use.datanode.hostname</name>
        <value>false</value>
    </property>

    <property>
         <name>dfs.namenode.http-address</name>
         <value>192.168.1.241:50071</value>
         <description>Your NameNode hostname for http access.</description>
    </property>

    <property>
         <name>dfs.namenode.secondary.http-address</name>
         <value>192.168.1.241:50090</value>
         <description>Your Secondary NameNode hostname for http access.</description>
    </property>
</configuration>

</xmp></pre>
					$HADOOP_CONF_DIR/yarn-site.xml
					<pre><xmp><configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>192.168.1.241</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    <property>
        <name>yarn.resourcemanager.resource-tracker.address</name>
        <value>192.168.1.241:8025</value>
    </property>
    <property>
        <name>yarn.resourcemanager.scheduler.address</name>
        <value>192.168.1.241:8026</value>
    </property>
    <property>
        <name>yarn.resourcemanager.address</name>
        <value>192.168.1.241:8050</value>
    </property>
</configuration>
</xmp></pre>

					<p>
                    <h5>4.3 &nbsp;:&nbsp;Remaining configuration</h5>
                    </p>
                    <p>
                    These modification are to be applied to both the namenodes as well as the data nodes</p>

                    $HADOOP_CONF_DIR/core-site.xml
                    <pre><xmp><configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://192.168.1.241/</value>
        <description>NameNode URI</description>
    </property>
</configuration></xmp></pre>

					$HADOOP_CONF_DIR/mapred-site.xml
					<pre><xmp><configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
</xmp></pre>
				$HADOOP_CONF_DIR/hadoop-env.sh
				<br>In this file, set the JAVA_HOME property appropriately.
				<pre>export JAVA_HOME=/usr/local/jdk1.8.0_45</pre>

				<p>Phew! That should do it.</p>
				TODO:
				<ul>
					<li>Testing</li>
					<li>Common examples</li>
					<li>accessing logs</li>
					<li>Common errors</li>
					<li>Debugging strategies</li> 
					<li>Sources references and code</li>
					<ul>

                  </div>
            </div>
        </div>
    </article>

    <hr>

      <!-- Footer -->
    <footer>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <ul class="list-inline text-center">
<!--                        <li>
                            <a href="http://www.facebook.com">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                        <li>
                            <a href="http://www.facebook.com">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
-->                       </li>
                        <li>
                            <a href="https://github.com/metastableB/bigDataExperiments">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                    </ul>
                    <p class="copyright text-muted">Don Dennis 2015</p>
                </div>
            </div>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="../../js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="../../js/bootstrap.min.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="../../js/clean-blog.min.js"></script>

</body>

</html>
